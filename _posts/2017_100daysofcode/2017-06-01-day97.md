---
layout: post
author: csiu
date: 2017-06-01
title: "Day97: Looking deep"
categories: update
tags:
  - 100daysofcode
  - kaggle
  - machine-learning
excerpt: Using deep learning
---

I saw this tweet today and wanted to see how deep learning compares with the decision trees in predicting [Kaggle House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).

<center>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Don&#39;t use deep learning your data isn&#39;t that big <a href="https://t.co/q9LZkUyoR5">https://t.co/q9LZkUyoR5</a> <a href="https://t.co/pi25EvZ8Tb">pic.twitter.com/pi25EvZ8Tb</a></p>&mdash; Simply Statistics (@simplystats) <a href="https://twitter.com/simplystats/status/869965857715257345">May 31, 2017</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

> In this tweet, ["Don't use deep learning your data isn't that big"](https://simplystatistics.org/2017/05/31/deeplearning-vs-leekasso/), SimplyStats (2017) says:
>
> "When your **dataset isnâ€™t that big**, doing something **simpler is often both more interpretable** and it works just as well due to potential overfitting."
>
> "For **low training set sample sizes** it looks like the simpler method (just picking the top 10 and using a linear model) **slightly outperforms** the more complicated methods. As the sample size increases, we see the more complicated method catch up and have comparable test set accuracy."

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kaggle/blob/master/house_prices/day97-deep.ipynb).

## Non-deep learning

In my previous posts, I predicted house prices using various decision trees with comparable accuracy.

| Approach | Day | Accuracy when "Margin of Error" is $5000 |
|-|:-:|:-:|
| Regression Tree | [Day93]({{ site.baseurl }}/update/2017/05/28/day93.html) | ~0.15 |
| Bagging | [Day94]({{ site.baseurl }}/update/2017/05/29/day94.html) | ~0.15 |
| Random Forest & Extremely Randomized Tree | [Day95]({{ site.baseurl }}/update/2017/05/30/day95.html) | ~0.25 |
| Boosting | [Day96]({{ site.baseurl }}/update/2017/05/31/day96.html) | ~0.11 |

*Note: The models have not been optimized.*

## Deep learning

Using deep learning, specified in Scikit-learn by [`MLPRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) (for regression) or `MLPClassifier` (for classification), we find:

```python
from sklearn.neural_network import MLPRegressor


mlp = MLPRegressor(alpha=0.001, max_iter=1000,
                   activation="relu", solver="lbfgs",
                   random_state=1)
mlp = mlp.fit(X_train, y_train)
```

*There are many parameter to tune, here we tuned 4 parameters.<br>(see the [Jupyter  Notebook](https://nbviewer.jupyter.org/github/csiu/kaggle/blob/master/house_prices/day97-deep.ipynb) for run settings)*

<img src="{{ site.baseurl }}/img/figure/2017-06-01/mlp-param.png" style="display: block; margin: auto; width: 100%" />

- Indeed predictions between the deep and non-deep methods are comparable
- changing the `alpha` values does not change the accuracy much
- 1000+ `epoch`s seem to give steady accuracies
- "relu" and "identity" `activation` does comparably better than "tanh" and "logistic"
- "sgd" `solver` does comparably bad
