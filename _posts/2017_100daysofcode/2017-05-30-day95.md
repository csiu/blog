---
layout: post
author: csiu
date: 2017-05-30
title: "Day95: Random Forest & Extremely Randomized Trees"
categories: update
tags:
  - 100daysofcode
  - kaggle
  - machine-learning
excerpt: More random trees
---

Similar to Bagging is **Random Forest**.

> **Bagging**<br>
> When samples are drawn with replacement (i.e., a bootstrap sample)
>
> **Random Forest**<br>
> When sample are drawn with replacement AND a random subset of candidate features is used

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kaggle/blob/master/house_prices/day95-random-forest-regression.ipynb).

### Random Forest

```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor()
rf = rf.fit(X_train, y_train)
```

In Python, Random Forest is specified by [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) (for classification) and [`RandomForestRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) (for regression).

### Extremely Randomized trees

In Extremely Randomized trees, the splitting threshold of features is randomized (on top of Random Forest) to further reduce the variance of the model. In other words:

> **Extremely Randomized trees**<br>
> When sample are drawn with replacement AND a random subset of candidate features is used AND the best of the randomly-generated thresholds is picked as the splitting rule

```python
from sklearn.ensemble import ExtraTreesRegressor

etr = ExtraTreesRegressor()
etr = etr.fit(X_train, y_train)
```

*For more information, see [Scikit-learn's "Extremely Randomized Trees"](http://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees).*

### Tree comparison

Using the same [Kaggle House Prices for Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) dataset as described on [Day93]({{ site.baseurl }}/update/2017/05/28/day93.html), we found that the models which use random subsets of features gave the most accurate predictions.

<img src="{{ site.baseurl }}/img/figure/2017-05-30/benchmark.png" style="display: block; margin: auto; width: 95%" />

*Note: Regression Tee was described on [Day93]({{ site.baseurl }}/update/2017/05/28/day93.html), Bagging on [Day94]({{ site.baseurl }}/update/2017/05/29/day94.html), and Random Forest & Extremely Randomized Tree on today.*
