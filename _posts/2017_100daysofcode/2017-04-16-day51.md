---
layout: post
author: csiu
date: 2017-04-16
title: "Day51: Basic SVD"
categories: update
tags:
  - 100daysofcode
  - machine-learning
excerpt: Singular value decomposition
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

One of the things you can do with a matrix is factorize it.

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kick/blob/master/src/ipynb/day51_svd.ipynb).

### Singular Value Decomposition (SVD)

[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) decomposes $$M$$ ($$m \times n$$ matrix) into 3 parts: $$U\Sigma{}V^*$$

  - $$U$$ is a matrix of size $$m \times m$$
  - $$\Sigma$$ is a diagonal matrix whose<br>diagonal entries are known as the singular values
  - $$V$$ is a matrix of size $$n \times n$$

<img src="{{ site.baseurl }}/img/figure/2017-04-16/svd.png" style="display: block; margin: auto; width:90%" />

  - **rank** refers to the maximum number of linearly independent vectors in a matrix

Once the matrix is factorized, one then reduces the rank of the matrix. This is done by removing singular values from the decomposed matrices (ie. keeping only the blue parts in the above figure) and then reconstructing/recombining the matrix. This loss of information from removing singular values will then produce a generalized matrix for which we can learn, compare with the original matrix, and make predictions from.

> Singular value decomposition is essentially trying to reduce a rank R matrix to a rank K matrix.
> - [What is an intuitive explanation of singular value decomposition (SVD)?](https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD) (Jason Liu, 2016)

### SVD in Python

Searching for how to run SVD in Python, I come across two implementations [`numpy.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) and [`scipy.linalg.svd`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html).


```python
import numpy as np
U, s, V = np.linalg.svd(M)
```
```python
import scipy.linalg
U, s, V = scipy.linalg.svd(M)
```

According to "[Why both numpy.linalg and scipy.linalg? Whatâ€™s the difference?](https://www.scipy.org/scipylib/faq.html#why-both-numpy-linalg-and-scipy-linalg-what-s-the-difference)", `scipy.linalg` is a more complete wrapping of Fortran LAPACK using f2py.

### SVD on the full matrix

The size of the [document-word count matrix from yesterday]({{ site.baseurl }}/update/2017/04/15/day50.html) is $$177140 \times 99946$$. When we run SVD on the full matrix, the job fails. The matrix is too big.

### SVD on a subset of the matrix

Here we take a subset (with only 10,000 documents and 100 words) and deconstruct the matrix with SVD:

```python
import scipy.linalg

# Subset M
M = M[:10000, :100]

# Run SVD
U, s, Vh = scipy.linalg.svd(x)

# U.shape, Vh.shape, s.shape
#> ((10000, 10000), (100, 100), (100,))
```

Next we plot the singular values of $$\Sigma$$:

```python
import seaborn as sns

f, ax = plt.subplots(figsize=(15,4))

sns.pointplot(y=s, x=list(range(1,len(s)+1)))

ax.set(xlabel="singular values", ylabel="eigenvalue")
```

<img src="{{ site.baseurl }}/img/figure/2017-04-16/sigma.png" style="display: block; margin: auto; width:99%" />

*There is information in the first 8 singular values.*

Finally we reconstruct the matrix with the top 8 singular values:

```python
numSV = 8

U = U[:,:numSV]
Vh = Vh[:numSV,:]
s = s[:numSV]

# U.shape, Vh.shape, s.shape
#> ((10000, 8), (8, 100), (8,))

# Reconstruct the original matrix
reconstructed = np.dot(np.dot(U,np.diag(s)), Vh)
```

### Future work
SVD runs on the smaller matrix, but errors on the full one. In the future work, I'll investigate how matrix decomposition is done with bigger matrices.
