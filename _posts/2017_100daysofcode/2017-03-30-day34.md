---
layout: post
author: csiu
date: 2017-03-30
title: "Day34: Getting the variable importance"
categories: update
tags:
  - 100daysofcode
  - machine-learning
excerpt: Using Garson's algorithm
---

*Note: The data used in today's posting is from the extracted MLP parameter weights of yesterday.*

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/100daysofcode/blob/master/nn/day34_mlp_important_features.ipynb).

### Inspecting the ANN model parameters

After loading the model parameters, we begin to look at (1) how many parameters are there and (2) what is the structure of these parameters.

    Number of params: 4
    -------------------
    (784, 100)   # Weights for input to hidden layer
    (100,)       # Bias weights to hidden layer
    (100, 10)    # Weights for hidden to output layer
    (10,)        # Bias weights to output layer

### Visualizing parameter weights between layers

We next plotted the parameter weights of each layer. Unsurprisingly, the weights are all over the place.

<img src="{{ site.baseurl }}/img/figure/2017-03-30/wt1.png" style="display: block; margin: auto; width: 49%" />
<img src="{{ site.baseurl }}/img/figure/2017-03-30/wt2.png" style="display: block; margin: auto; width: 49%" />

*TOP: weights of input to hidden layer;*\\
*BOTTOM: weights of hidden to output layer.*

### Visualizing the parameter weights across input features

<img src="{{ site.baseurl }}/img/figure/2017-03-30/box.png" style="display: block; margin: auto; width: 95%" />

When we visualize the parameter weights to the hidden layer by input feature, we find that some input features are more variable in terms of parameter weights than others.

### Garson's algorithm to calculate variable importance

Using Garson's algorithm to calculate the feature importance for each of the different output classes, we get the following figure...

<img src="{{ site.baseurl }}/img/figure/2017-03-30/garson.png" style="display: block; margin: auto; width: 95%" />

... which shows the feature importance as being identical across the output classes. When we consider the math, this property makes sense:

<img src="{{ site.baseurl }}/img/figure/2017-03-30/proof.png" style="display: block; margin: auto; width: 95%" />

### Getting the important features

Finally, we are able to sort by the variable importance for the important features.

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>relative_importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>69</th>
      <td>0.001629</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.001550</td>
    </tr>
    <tr>
      <th>502</th>
      <td>0.001537</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.001526</td>
    </tr>
    <tr>
      <th>530</th>
      <td>0.001520</td>
    </tr>
    <tr>
      <th>248</th>
      <td>0.001511</td>
    </tr>
    <tr>
      <th>417</th>
      <td>0.001510</td>
    </tr>
    <tr>
      <th>678</th>
      <td>0.001506</td>
    </tr>
    <tr>
      <th>401</th>
      <td>0.001504</td>
    </tr>
    <tr>
      <th>734</th>
      <td>0.001495</td>
    </tr>
  </tbody>
</table>
</div>
