---
layout: post
author: csiu
date: 2017-03-08
title: "Day12: Visualizing high-dimensional data: TSNE & PCA"
categories: update
tags:
  - 100daysofcode
  - data-mining
  - text-mining
  - visualization
excerpt: Looking at League of Legends data
---

I want to continue from where I left off (ie. visualizing [League of Legends: Base champion statistics](http://leagueoflegends.wikia.com/wiki/Base_champion_statistics)).

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/100daysofcode/blob/master/lol/visualizing-higher-dimensions.ipynb).


### Scraping 134 more web pages for "Primary role"

Yesterday, I scraped only 1 page. Today, I will scrape 134 more to obtain the "Primary role" information of each League of Legends champion. This data is made available on [Gist](https://gist.github.com/csiu/06f3d276a608aeb6212744e49db2a7a4).


```python
from bs4 import BeautifulSoup
import requests

primary_role = []
for url in dat.href:
    html_data = requests.get(url).text
    soup = BeautifulSoup(html_data, "html5lib")

    role = soup.find('div', attrs={'class' : 'champion_info'}).table.a.text
    primary_role.append(role)

dat["primary_role"] = primary_role

# Save data
dat.to_csv("lol_base_stats-roles.tsv", index=False, sep="\t")
```

When we plot the distribution of primary roles, we find most champions are Fighters.

<img src="{{ site.baseurl }}/img/figure/2017-03-08/day12_01.png" style="display: block; margin: auto; width: 100%" />

### t-distributed Stochastic Neighbor Embedding (TSNE)

```python
from sklearn.manifold import TSNE

model = TSNE(n_components=2, random_state=0)
X = model.fit_transform(data)
```

There are many dimensions to the data (eg. HP, HP+, HP5, HP5+, MP, ...). We can therefore use t-SNE for visualization. More information about t-SNE is available at [Laurens van der Maaten's Github page](https://lvdmaaten.github.io/tsne/). According to Scikit-learn:

> [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.

When we plot the t-SNE plot at different perplexities, we get the following plots:

<img src="{{ site.baseurl }}/img/figure/2017-03-08/day12_02.png" style="display: block; margin: auto; width: 100%" />

### Principal component analysis (PCA)

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca.fit(datc)
X = pca.transform(datc)
```

PCA, a dimensionality reduction algorithm, can also be used to explore variation in a data set.

> Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used to make data easy to explore and visualize ([Victor Powell](http://setosa.io/ev/principal-component-analysis/))

<img src="{{ site.baseurl }}/img/figure/2017-03-08/day12_03.png" style="display: block; margin: auto; width: 60%" />

From this plot, we see that the Mage and Marksman champions are more similar to each other than to Fighters, Tanks, and Slayers. Consulting my brother, he said the Mage and Marksman champions do tend to fight at a range.
