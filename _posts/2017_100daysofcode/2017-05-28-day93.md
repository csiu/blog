---
layout: post
author: csiu
date: 2017-05-28
title: "Day93: Regression trees"
categories: update
tags:
  - 100daysofcode
  - kaggle
  - machine-learning
excerpt: Decision trees applied to regression problems
---

I always thought decision trees were meant to be applied problems with categorical outcomes, but I was mistaken. Decision trees where the target variable can take continuous values are called **regression trees**.

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kaggle/blob/master/house_prices/day93-regression-trees.ipynb)


### Dataset: Predicting House Prices

The data explored in this post is the [Kaggle House Prices for Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) dataset, where the goal is to predict sales prices. But before training the regression tree, the data was first preprocessed by:

1. Mapping categorical variables to numerical variables using one-hot encoding (as done in [Day24]({{ site.baseurl }}/update/2017/03/20/day24.html))
2. Removing NAs
3. Splitting the data to 80% training and 20% validation

### Regression tree

Once the training data is preprocessed, we can then train a regression tree with:

```python
from sklearn import tree

clf = tree.DecisionTreeRegressor()
clf = clf.fit(X_train, y_train)

# Prediction
clf.predict(X_test)
```

### Evaluation

Evaluating the predictions on the validation set, we get 1% prediction accuracy.

*Very low.*

If instead the prediction does not need to be spot on (i.e. exact), the prediction accuracy is much better.

<img src="{{ site.baseurl }}/img/figure/2017-05-28/reg-tree-eval.png" style="display: block; margin: auto; width: 95%" />

*Keep in mind we have not cross validated or done any feature engineering.*
