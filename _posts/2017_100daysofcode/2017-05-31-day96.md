---
layout: post
author: csiu
date: 2017-05-31
title: "Day96: Boosting"
categories: update
tags:
  - 100daysofcode
  - kaggle
  - machine-learning
excerpt: Boosting = sequential learning of weak learners
---

Another approach for improving the predictions resulting from a decision tree is **Boosting**.

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kaggle/blob/master/house_prices/day96-boosting.ipynb).

## Boosting

In boosting, trees are grown sequentially. This means learning depends strongly on trees that already have been grown. According to the [Introduction to Statistical Learning (p.321 "Boosting")](http://www-bcf.usc.edu/~gareth/ISL/), *Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.*

In Scikit-learn, a popular boosting algorithm "AdaBoost" is called using [`sklearn.ensemble.AdaBoostClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).

[<img src="{{ site.baseurl }}/img/figure/2017-05-31/scikit-boost.png" style="display: block; margin: auto; width: 95%" />](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)

```python
from sklearn.ensemble import AdaBoostClassifier

boost = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=1)
boost = boost.fit(X_train, y_train)
```

## Tuning the boosting algorithm

With regards to parameters, Boosting has 3:

1. Number of trees (too many => overfit)
2. Shrinkage parameter (~learning rate)
3. Number of splits in tree (~complexity of model; weak learners okay)

----

```python
# Try these parameters
number_of_trees = [25, 50, 100, 200]
learning_rate = [1, 0.1, 0.01, 0.001]
```

When we varied the `number_of_trees` and `learning_rate` of the boosting algorithm on the [Kaggle House Prices for Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) dataset as described on [Day93]({{ site.baseurl }}/update/2017/05/28/day93.html), we found that:

<img src="{{ site.baseurl }}/img/figure/2017-05-31/compare2.png" style="display: block; margin: auto; width: 95%" />

- more trees => better predictors
- low learning rates => slow learning => poor predictors  [Right panel]
- high learning rates => learning not precise => poor predictors [Left panel]

*Sweet spot is in between.*
