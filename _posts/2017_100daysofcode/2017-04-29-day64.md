---
layout: post
author: csiu
date: 2017-04-29
title: "Day64: TD-IDF"
categories: update
tags:
  - 100daysofcode
  - kaggle
excerpt: Term Frequency-Inverse Document Frequency
header-img: img/header/2017-04-29_portland.jpg
header-img-source: Celia Siu (in Portland)
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

Today I look at another presentation of words in documents. Previously, I used [raw counts]({{ site.baseurl }}/update/2017/04/15/day50.html) and today I consider TD-IDF.

The associated modifications to the Python script is found [here](https://github.com/csiu/kick/commit/5210ebe8b4782667836a335e38e7983a04d7d3dd).

### TF-IDF

[TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) representation is another representation of words in documents.  TF-IDF stands for Term Frequency-Inverse Document Frequency and is intended to reflect how important a word is to a document in a corpus. A word that appears in the document more times is considered more relevant to the document; but if that word appears more frequently in general (words such as “the”, “if”, or “a”), then this word might not be that relevant.

- Reference: [Tf–idf term weighting](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)

<br>

### TF-IDF in Python

In Python (ie. scikit-learn), TF-IDF is computed by [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) (from raw text) or [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) (from a count matrix).

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
mat = vectorizer.fit_transform(list_of_documents)


# Print array
mat.toarray()
```
