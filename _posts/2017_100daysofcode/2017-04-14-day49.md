---
layout: post
author: csiu
date: 2017-04-14
title: "Day49: Text preprocessing"
categories: update
tags:
  - 100daysofcode
excerpt: Data cleaning and text preprocessing
---

I eventually want to do text analysis with the Kickstarter data, but first I'll need to do some data cleaning and text preprocessing.

The Jupyter Notebook for this little project is found [here](https://nbviewer.jupyter.org/github/csiu/kick/blob/master/src/ipynb/day49_text_preprocessing.ipynb).

## Select data for analysis

The data for this project [<i class="fa fa-github"></i>](https://github.com/csiu/kick) has been [loaded to a PostgreSQL database]({{ site.baseurl }}/update/2017/04/08/day43.html). The columns of the database can be found on the [Jupyter Notebook of Day44](https://nbviewer.jupyter.org/github/csiu/100daysofcode/blob/master/misc/day44_querying_database.ipynb). From the data, I want to use columns "name" and "blurb".

<div>
<table border="0" class="dataframe" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>name</th>
      <th>blurb</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1312331512</td>
      <td>Otherkin The Animated Series</td>
      <td>We have a fully developed 2D animated series t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>80827270</td>
      <td>Paradigm Spiral - The Animated Series</td>
      <td>A sci-fi fantasy 2.5D anime styled series abou...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>737219121</td>
      <td>I'm Sticking With You.</td>
      <td>A film created entirely out of paper, visual e...</td>
    </tr>
  </tbody>
</table>
</div>

Both columns are text fields and I want to combine and treat them as a single "document".

> document - collection of text

 In PostgreSQL, you can combine/concatenate rows with a whitespace separator using the `concat_ws` command.

```sql
SELECT id, concat_ws(name, blurb) FROM info
```

<div>
<table border="0" class="dataframe" style="margin-bottom: 2em;" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1312331512</td>
      <td>We have a fully developed 2D animated series t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>80827270</td>
      <td>A sci-fi fantasy 2.5D anime styled series abou...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>737219121</td>
      <td>A film created entirely out of paper, visual e...</td>
    </tr>
  </tbody>
</table>
</div>

## Text preprocessing

In text preprocessing, we do the following:

```sh
# Given some text:
'A sci-fi fantasy 2.5D anime styled series about some guys trying to save the world, probably...'

# 1. Convert text to lowercase
'a sci-fi fantasy 2.5d anime styled series about some guys trying to save the world, probably...'

# 2. Remove non-letter (such as digits)
'a scifi fantasy d anime styled series about some guys trying to save the world probably'

# 3. Tokenize (ie. break text into meaningful elements such as words)
    ['a',
     'scifi',
     'fantasy',
     'd',
     'anime',
     'styled',
     'series',
     'about',
     'some',
     'guys',
     'trying',
     'to',
     'save',
     'the',
     'world',
     'probably']

# 4. Remove stopwords
    ['scifi',
     'fantasy',
     'anime',
     'styled',
     'series',
     'guys',
     'trying',
     'save',
     'world',
     'probably']
```

In Python's `nltk.corpus`, there are 153 english stopwords which we can use in filtering for words with significance.

> stopwords - words which do not contain important significance

    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']

- Reference to remove stop words: [Part 1: For Beginners - Bag of Words](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words)
- Reference for [How do I do word Stemming or Lemmatization?](http://stackoverflow.com/questions/771918/how-do-i-do-word-stemming-or-lemmatization)

### Stemming vs Lemmatizing

At this point, there are various things we can do to the data including stemming and lemmatizing which would allow us to treat "greets", "greet", and "greeting" as the same word.

> [stemming](https://en.wikipedia.org/wiki/Stemming) - process of reducing inflected (or sometimes derived) words to their word stem, base or root form

> [lemmatizing](https://en.wikipedia.org/wiki/Lemmatisation) - process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form

Applying the `PorterStemmer` and `WordNetLemmatizer` to the example, we find:

<div>
<table border="0" class="dataframe" style="margin-bottom: 1em;" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>raw</th>
      <th>stemming</th>
      <th>lemmatizing</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>scifi</td>
      <td>scifi</td>
      <td>scifi</td>
    </tr>
    <tr>
      <th>1</th>
      <td>fantasy</td>
      <td>fantasi</td>
      <td>fantasy</td>
    </tr>
    <tr>
      <th>2</th>
      <td>anime</td>
      <td>anim</td>
      <td>anime</td>
    </tr>
    <tr>
      <th>3</th>
      <td>styled</td>
      <td>style</td>
      <td>styled</td>
    </tr>
    <tr>
      <th>4</th>
      <td>series</td>
      <td>seri</td>
      <td>series</td>
    </tr>
    <tr>
      <th>5</th>
      <td>guys</td>
      <td>guy</td>
      <td>guy</td>
    </tr>
    <tr>
      <th>6</th>
      <td>trying</td>
      <td>tri</td>
      <td>trying</td>
    </tr>
    <tr>
      <th>7</th>
      <td>save</td>
      <td>save</td>
      <td>save</td>
    </tr>
    <tr>
      <th>8</th>
      <td>world</td>
      <td>world</td>
      <td>world</td>
    </tr>
    <tr>
      <th>9</th>
      <td>probably</td>
      <td>probabl</td>
      <td>probably</td>
    </tr>
  </tbody>
</table>
</div>

1. Some words are untouched:
    - scifi
    - save
    - world
2. Some words are touched only in stemming:
    - fantsy-fantasi
    - anime->anim
    - styled->style
    - series->seri
    - trying->tri    
    - probably->probabl
3. Some words agree in its stemming and lemmatizing:
    - guys->guy

## Putting it all together

```python
def text_processing(text, method=None):
    # Lower case
    text = text.lower()

    # Remove non-letters &
    # Tokenize    
    words = nltk.wordpunct_tokenize(re.sub('[^a-zA-Z_ ]', '', text))

    # Remove stop words
    words = [w for w in words if not w in stopwords.words("english")]

    # Stemming vs Lemmatizing vs do nothing
    if method == "stem":
        port = PorterStemmer()
        words = [port.stem(w) for w in words]
    elif method == "lemm":
        wnl = WordNetLemmatizer()
        words = [wnl.lemmatize(w) for w in words]

    return(words)
```
